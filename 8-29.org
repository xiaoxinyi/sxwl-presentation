* This Week
** [[https://github.com/BVLC/caffe/tree/85bb397acfd383a676c125c75d877642d6b39ff6/examples/feature_extraction][extract feature]]
*** using caffe to extract features
     #+BEGIN_SRC sh
       find `pwd`/examples/images -type f -exec echo {} \; > examples/_temp/temp.txt
       sed "s/$/ 0/" examples/_temp/temp.txt > examples/_temp/file_list.txt
       cd $CAFFE
       ./build/tools/extract_features models/bvlc_reference_caffenet/bvlc_reference examples/_temp/imagenet_val.prototxt example/_temp/feature fc7 10 lmdb GPU 0
     #+END_SRC
*** general command for extract feature using caffe
#+BEGIN_SRC sh
  extract_features pretrained_net_param  feature_extraction_proto_file \
  extract_feature_blob_name1[,name2,...]  save_feature_dataset_name1[,name2,...] \
  num_mini_batches  db_type  [CPU/GPU] [DEVICE_ID=0]
#+END_SRC
- 参数1是模型参数（.caffemodel）文件的路径。

- 参数2是描述网络结构的prototxt文件。程序会从参数1的caffemodel文件里找
  对应名称的layer读取参数。 

- 参数3是需要提取的blob名称，对应网络结构prototxt里的名称。blob名称可
  以有多个，用逗号分隔。每个blob提取出的特征会分开保存。 

- 参数4是保存提取出来的特征的数据库路径，可以有多个，和参数3中一一对应，
  以逗号分隔。如果用LMDB的话，路径必须是不存在的（已经存在的话要改名或
  者删除）。  

- 参数5是提取特征所需要执行的batch数量。这个数值和prototxt里DataLayer
  中的Caffe的DataLayer(或者ImageDataLayer)中的batch_size参数相乘，就是
  会被输入网络的总样本数。设置参数时需要确保batch_size *
  num_mini_batches等于需要提取特征的样本总数，否则提取的特征就会不够数
  或者是多了。  

- 参数6是保存特征使用的数据库类型，支持lmdb和leveldb两种(小写)。推荐使
  用lmdb，因为lmdb的访问速度更快，还支持多进程同时读取。  

- 参数7决定使用GPU还是CPU，直接写对应的三个大写字母就行。省略不写的话
  默认是CPU。  

- 参数8决定使用哪个GPU，在多GPU的机器上跑的时候需要指定。省略不写的话
  默认使用0号GPU。  

注意事项
- 提取特征时，网络运行在Test模式下
    * Dropout层在Test模式下不起作用，不必担心dropout影响结果
    * Train和Test的参数写在同一个Prototxt里的时候，改参数的时候注意不要改错地方(比如有两个DataLayer的情况下) 
- 减去均值图像
    * 提取特征时，输入的图像要减去均值
    * 应该减去训练数据集的均值
- 提取哪一层
    * 不要提取Softmax网络的最后一层(如AlexNet的fc8)，因为最后一层已经是分类任务的输出，作为特征的可推广性不够好
*** read from lmdb
     #+BEGIN_SRC python
       import numpy as np
       import caffe
       import lmdb
       from caffe.proto import caffe_pb2

       fea_lmdb = lmdb.open('featureA')
       lmdb_txn = fea_lmdb.begin()
       lmdb_cursor = txn.cursor()
       features = []

       for key, value in lmdb_cursor:
           datum = caffe_pb2.Datum()
           datum.ParseFromString(value)
           data = caffe.io.datum_to_array(datum)
           features.append(data)

     #+END_SRC
*** image recognition using =cos= similarity measure
#+BEGIN_SRC python

  import numpy as np
  import caffe
  import lmdb
  from caffe.proto import caffe_pb2
  from scipy import spatial


  # 3 steps to read form lmdb
  fea_lmdb = lmdb.open('/root/caffe/examples/_temp/featureA')
  lmdb_txn = fea_lmdb.begin()
  lmdb_cursor = lmdb_txn.cursor()
  features = []

  for key, value in lmdb_cursor:
      datum = caffe_pb2.Datum()
      # Parse from serialized data
      datum.ParseFromString(value)
      data = caffe.io.datum_to_array(datum)
      features.append(data)

  out = []
  for f in features:
      out.append(f.flatten())

  n = len(out)
  similarity = np.zeros((n, n), dtype=np.double)

  for i in xrange(n):
      for j in xrange(n):
        # cosin distance
          similarity[i, j] = 1 - spatial.distance.cosine(out[i], out[j])

#+END_SRC
*** =cos= similarity result
- accuracy (true ture) : 53 / 55
#+BEGIN_SRC python
a = similarity[0:10, 0:10]
  array([[ 1.        ,  0.63231419,  0.84345085,  0.73587363,  0.58211244,
           0.67306891,  0.46881317,  0.56938226,  0.65432654,  0.55240935],
         [ 0.63231419,  1.        ,  0.68508232,  0.56741804,  0.74116358,
           0.81706845,  0.71951714,  0.75391089,  0.78529276,  0.74174079],
         [ 0.84345085,  0.68508232,  1.        ,  0.78416825,  0.61635946,
           0.72695667,  0.54473343,  0.60050371,  0.70046374,  0.58715887],
         [ 0.73587363,  0.56741804,  0.78416825,  1.        ,  0.50801387,
           0.60814318,  0.5046651 ,  0.52948304,  0.68054069,  0.49502061],
         [ 0.58211244,  0.74116358,  0.61635946,  0.50801387,  1.        ,
           0.88589477,  0.56183335,  0.72687896,  0.60917844,  0.87135289],
         [ 0.67306891,  0.81706845,  0.72695667,  0.60814318,  0.88589477,
           1.        ,  0.63597132,  0.76000156,  0.7042399 ,  0.87401555],
         [ 0.46881317,  0.71951714,  0.54473343,  0.5046651 ,  0.56183335,
           0.63597132,  1.        ,  0.58212342,  0.64319046,  0.6254508 ],
         [ 0.56938226,  0.75391089,  0.60050371,  0.52948304,  0.72687896,
           0.76000156,  0.58212342,  1.        ,  0.74652927,  0.72233884],
         [ 0.65432654,  0.78529276,  0.70046374,  0.68054069,  0.60917844,
           0.7042399 ,  0.64319046,  0.74652927,  1.        ,  0.61672591],
         [ 0.55240935,  0.74174079,  0.58715887,  0.49502061,  0.87135289,
           0.87401555,  0.6254508 ,  0.72233884,  0.61672591,  1.        ]])

np.sum(a > 0.5)
96
#+END_SRC
- false true : 2 / 100
#+BEGIN_SRC python
In [1]: ab = similarity[0:10, 10:]

In [2]: ab
Out[2]:
array([[ 0.2842583 ,  0.37596221,  0.27628312,  0.12041221,  0.29636999,
         0.13618284,  0.1381707 ,  0.17832465,  0.21937008,  0.40752771],
       [ 0.32961919,  0.49064045,  0.29595205,  0.093565  ,  0.39657901,
         0.17370467,  0.15514055,  0.2672414 ,  0.31652746,  0.46922921],
       [ 0.31926577,  0.45413662,  0.26234978,  0.1560283 ,  0.30816957,
         0.15273065,  0.16850629,  0.22604249,  0.25764858,  0.44164225],
       [ 0.26623039,  0.3611369 ,  0.20121232,  0.11351721,  0.21726182,
         0.11916629,  0.1431136 ,  0.20710409,  0.22387793,  0.31652456],
       [ 0.30927462,  0.35910132,  0.2650208 ,  0.08663475,  0.37263798,
         0.10722143,  0.09815253,  0.17950735,  0.20988739,  0.50689106],
       [ 0.32089366,  0.40492257,  0.28595893,  0.09466663,  0.37709065,
         0.10737807,  0.10595637,  0.19340299,  0.23139416,  0.51704389],
       [ 0.29795872,  0.3890121 ,  0.26349005,  0.08589599,  0.36945176,
         0.16923292,  0.11844475,  0.24970864,  0.31689723,  0.36337912],
       [ 0.28911623,  0.33516171,  0.30897566,  0.12046317,  0.36436887,
         0.10022814,  0.14957088,  0.29092572,  0.3343103 ,  0.47673998],
       [ 0.31926479,  0.43550698,  0.31588098,  0.09185497,  0.33737191,
         0.15741605,  0.16819127,  0.34134218,  0.38785466,  0.41883917],
       [ 0.29190126,  0.3130953 ,  0.25801771,  0.07097081,  0.34608239,
         0.09577894,  0.0842366 ,  0.14185045,  0.19112799,  0.47368384]])

In [3]: np.sum(ab > 0.5)
Out[3]: 2

#+END_SRC

** Cuda Note
*** Configuring the kernel launch
=kernel<<<grid of block, block of threads>>>(...)=

=square<<<dim3(bx,by,bz), dime(tx,ty,tz), sharem>>>(...)=

  * grid of blocks : bx * by * bz
  * block of threads : tx * ty * tz
  * shared memory per block in bytes
*** Convert color to black and white
    #+BEGIN_SRC example
      I = (R + G + B) / 3
      I = .299f * R + .587f * G + .114f * B
    #+END_SRC
*** [[http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#cuda-programming-model][ =nvcc= introduction]]
*** cs344 Note
- GPU is responsible for allocating blocks to SM(streaming multiprocessors)
- A block cannot run on more than one SM
- An SM may run more than one block
- All the SMs are running in parallel
- Threads in different block shouldn't cooperate
- Cuda make few guarantees about when and where thread blocks will run
- consequences
  + no assumptions blocks -> SM
  + no communication between blocks
- CUDA guarantees that:
  + all threads in a block run on the same SM at the same time
  + all blocks in a kernel finish before any blocks from next run
- threadIdx : thread within block threadIdx.x threadIdx.y
  + blockDim : size of block
  + blockIdx : block within grid
  + gridDim : size of grid
*** GPU memory model

[[./images/gpu-memory-model.png]]
  * All threads from a block can access the same variable in that
    block shared memory
  * Threads from two different blocks can access the same variable in
    global memory
  * Threads from different blocks have their own copy of local
    variables in local memory
  * Threads from the same block have their own copy of local variables
    in local memory

*** barrier
point in program where threads stop and wait. when all threads have
reached the barrier, they can proceed.

[[./images/synchronized.png]]
*** High-level strategies
  * Maximize arithmetic intensity

\[ \frac{Math}{Memory} \]

    - maximize compute ops per thread
    - minimize time spent on memory per thread
       + move frequently-accessed data to fast memory
#+BEGIN_SRC example
          local > shared >> global >> cpu memory
#+END_SRC

  * coalesce memeory

[[./images/coalesce.png]]

  * avoid thread divergence

*** =cudaMalloc=
    #+BEGIN_SRC c++
      float *device_data=NULL;  
      size_t size = 1024*sizeof(float);  
      cudaMalloc((void**)&device_data, size);  
    #+END_SRC
而 =device_data= 这个指针是存储在主存上的。之所以取 =device_data= 的地
址，是为了将 =cudaMalloc= 在显存上获得的数组首地址赋值给 =device_data=
。在函数中为形参赋值是不会在实参中繁盛变化的，但是指针传递的是地址 

*** TODO [[/Users/zhangli/Documents/Library.papers3/Files/1E/1ED49076-5D40-4E5F-B232-918B17EA1596.pdf][What Every Programmer Should Know About Memory]]
    SCHEDULED: <2016-08-27 Sat>
    
* Next Week
** Cuda programming
** caffe 
