* This week
- 了解caffe中
- 了解RNN和LSTM
- Why =convolution=
kernel/filter matrix 有滤镜作用


[[./convolution.png]]


99 imagenet filters
[[./imagenet.png]]


Notice that if all neurons in a single depth slice are using the same
weight vector, then the forward pass of the CONV layer can in each
depth slice be computed as a convolution of the neuron’s weights with
the input volume (Hence the name: Convolutional Layer). This is why it
is common to refer to the sets of weights as a filter (or a kernel),
that is convolved with the input.


It turns out that we can dramatically reduce the number of parameters
by making one reasonable assumption: That if one feature is useful to
compute at some spatial position (x,y), then it should also be useful
to compute at a different position (x2,y2). 

If detecting a horizontal edge is important at some location in the
image, it should intuitively be useful at some other location as well
due to the translationally-invariant structure of images. 
- extract features from cnn
- 配置 =torch= 
* Next week
- 继续学习caffe
- 利用 =imagenet= 训练好的模型，在caffe框架下提取图片的特征
